{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "getdata.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3jIxpc-OpIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests,re,time\n",
        "from lxml import etree\n",
        "import os,threading\n",
        "import fnmatch\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCuWew_EOsuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class sub_spyder():\n",
        "  def __init__(self,pages,download_path='./zip/'):\n",
        "    self.domain_name = \"https://subscene.com\"\n",
        "    self.cookie = 'LanguageFilter=27; HearingImpaired=2; ForeignOnly=False; _gat=1;'\n",
        "    self.pages = pages\n",
        "    self.dp = download_path\n",
        "    assert self.pages >= 2 and self.pages <= 78\n",
        "    \n",
        "    try:\n",
        "      os.mkdir(self.dp)\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  def get_pages(self):\n",
        "    pagelist = r\"{}/browse/latest/all/\".format(self.domain_name)\n",
        "\n",
        "    for i in range(2,self.pages+1):\n",
        "      page = \"{}{}\".format(pagelist,i)\n",
        "      yield page\n",
        "\n",
        "  def find_download_page(self):\n",
        "    headers = {\"cookie\":self.cookie}\n",
        "    for page in self.get_pages():\n",
        "      r = requests.get(page,headers=headers)\n",
        "      time.sleep(3)\n",
        "      xmlcontent = etree.HTML(r.text)\n",
        "      dls = xmlcontent.xpath(\"//td[@class='a1']/a/@href\")\n",
        "      for dl in dls:\n",
        "        download = \"{}{}\".format(self.domain_name,dl)\n",
        "        # print(download)\n",
        "        yield download\n",
        "      print(\"on page {}\\nget {} sites\".format(page,len(dls)))\n",
        "\n",
        "  def get_download_link(self):\n",
        "    for i in self.find_download_page():\n",
        "      r = requests.get(i)\n",
        "      xmlcontent = etree.HTML(r.text)\n",
        "      downloadlink = xmlcontent.xpath(\"//div[@class='download']/a/@href\")[0]\n",
        "      downloadlink = \"{}{}\".format(self.domain_name,downloadlink)\n",
        "      r_head = requests.head(downloadlink).headers\n",
        "      if 'Content-Disposition' in r_head:\n",
        "        content = r_head.get('Content-Disposition')\n",
        "        filename =  re.findall('(?<=filename=).*',content)[0]\n",
        "\n",
        "      yield (downloadlink,filename)\n",
        "\n",
        "  def downloader(self):\n",
        "    for dl,fn in self.get_download_link():\n",
        "      r = requests.get(dl,stream=True)\n",
        "      with open(self.dp+fn,'wb+') as f:\n",
        "        for chunk in r.iter_content(chunk_size=1024):\n",
        "          if chunk:\n",
        "            f.write(chunk)\n",
        "      time.sleep(1)\n",
        "      # print(\"save as {}\".format(fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkg0lTE75SF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class process_file():\n",
        "  def __init__(self):\n",
        "    self.path = \"./zip/\"\n",
        "    self.zip_out = \"./zip_out/\"\n",
        "\n",
        "    self.mkdir()\n",
        "    self.unzip()\n",
        "    self.conv_name()\n",
        "    self.normalize()\n",
        "    \n",
        "  def unzip(self):\n",
        "    for root,dirs,files in os.walk(self.path):\n",
        "      for filename in fnmatch.filter(files,\"*.zip\"):\n",
        "        cmd = \"unzip {} -d {}\".format(root+filename,self.zip_out)\n",
        "        # print(cmd)\n",
        "        os.system(cmd)\n",
        "  \n",
        "  def conv_name(self):\n",
        "    index = 1\n",
        "    for root,dirs,files in os.walk(self.zip_out):\n",
        "      for filename in fnmatch.filter(files,\"*.srt\"):\n",
        "        newfile = \"{:0>4}.srt\".format(index)\n",
        "        # print(filename,\"============\",newfile)\n",
        "        shutil.move(root+filename,self.zip_out+newfile)\n",
        "        index +=1\n",
        "\n",
        "  # def remove_charset_err(self):\n",
        "  #   for root,dirs,files in os.walk(self.zip_out):\n",
        "  #     for filename in fnmatch.filter(files,\"*.srt\"):\n",
        "  #       file_path = root+filename\n",
        "  #       try:\n",
        "  #         with open(file_path,'r+',encoding='utf-8') as f:\n",
        "  #           f.read()\n",
        "  #           pass\n",
        "  #       except:\n",
        "  #         print(\"charset err {}\".format(file_path))\n",
        "  #         os.remove(file_path)\n",
        "\n",
        "\n",
        "  def normalize(self):\n",
        "    for root,dirs,files in os.walk(self.zip_out):\n",
        "      for filename in fnmatch.filter(files,\"*.srt\"):\n",
        "        file_path = root+filename\n",
        "        # print(file_path)\n",
        "        try:\n",
        "          with open(file_path) as fr:\n",
        "            content = fr.read()\n",
        "            content = re.sub(r'\\d(.*?)\\d\\n','',content,flags=re.M) #除时间轴\n",
        "            content = re.sub(r'\\d\\n','',content,flags=re.M) #除序列\n",
        "            content = re.sub(r'NETF.*|.*日本語字幕.*','',content,flags=re.M) #除文字信息\n",
        "            content = re.sub(r'\\(.*?\\)|（.*?）|\\{(.*?)\\}','',content,flags=re.M) #除特殊符号\n",
        "            content = content.replace('\\n\\n','\\n') #除多余换行\n",
        "            # print(content)\n",
        "\n",
        "          with open(file_path,\"w+\") as fw:\n",
        "            fw.write(content)\n",
        "            # print(file_path)\n",
        "        except Exception as E:\n",
        "          #unicode decode error,need fix\n",
        "          print(E)\n",
        "          print('charset err path:%s' % file_path)\n",
        "\n",
        "  def mkdir(self):\n",
        "    try:\n",
        "      os.mkdir(self.zip_out)\n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4U3S56TWsAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(pages=70):\n",
        "  a = sub_spyder(pages)\n",
        "  start = time.time()\n",
        "  a.downloader()\n",
        "  cost = time.time()-start\n",
        "  print('{:.2f}s'.format(cost))\n",
        "\n",
        "  start = time.time()\n",
        "  b = process_file()\n",
        "  cost = time.time()-start\n",
        "  print('{:.2f}s'.format(cost))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGRjbbvvXp4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbXU8o3ZYYZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls ./zip_out"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}